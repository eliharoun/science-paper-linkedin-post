[
  {
    "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
    "authors": [
      "Shang Yang",
      "Junxian Guo",
      "Haotian Tang",
      "Qinghao Hu",
      "Guangxuan Xiao",
      "Jiaming Tang",
      "Yujun Lin",
      "Zhijian Liu",
      "Yao Lu",
      "Song Han"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14866v1",
    "abstract": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
    "arxiv_id": "2502.14866v1",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "format": "standard",
    "key_findings": "Model Llama-3-8B Llama-2-7B Benchmark Dense LServe Dense LServe 2WikiMQA 30.3 31.6 35.4 35.1 DuReader 30.3 30.8 25.4 24.7 HotpotQA 41.7 42.7 47.4 49.6 MultiNews 27.7 27.7 26.6 26.6 Qasper 31.7 29.3 32.6 29.5 QMSum 23.8 24.0 21.0 21.3 SamSum 41.2 39.3 41.8 41.5 TriviaQA 84.9 83.7 86.2 86.5\nExperi- ments reveal that LServe-8192 is only up to 6% slower than LServe-4096 when the sequence length exceeds 128K.\nLlama-3-8B 32K 64K 128K 160K 192K 256K Dense 90.5 86.8 83.8 79.3 79.6 79.4 LServe-4096 91.0 85.6 81.0 79.0 76.1 75.7 LServe-8192 91.8 86.1 81.7 81.2 79.7 79.1 the prefilling stage, we use  time-to-first-token  (TTFT) as a key metric, while for the decoding stage, we emphasize minimizing the  per-token generation latency .",
    "technical_innovation": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage.\nTo address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention.",
    "practical_applications": "No explicit applications extracted",
    "impact_analysis": "No explicit impact statements extracted",
    "pdf_file": "output/paper_cache/2502.14866v1.pdf"
  },
  {
    "title": "Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts",
    "authors": [
      "Sara Ghaboura",
      "Ketan More",
      "Ritesh Thawkar",
      "Wafa Alghallabi",
      "Omkar Thawakar",
      "Fahad Shahbaz Khan",
      "Hisham Cholakkal",
      "Salman Khan",
      "Rao Muhammad Anwer"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14865v1",
    "abstract": "Understanding historical and cultural artifacts demands human expertise and\nadvanced computational techniques, yet the process remains complex and\ntime-intensive. While large multimodal models offer promising support, their\nevaluation and improvement require a standardized benchmark. To address this,\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\n266 distinct cultures across 10 major historical regions. Designed for\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\nframework to assess AI models' capabilities in classification, interpretation,\nand historical comprehension. By integrating AI with historical research,\nTimeTravel fosters AI-powered tools for historians, archaeologists,\nresearchers, and cultural tourists to extract valuable insights while ensuring\ntechnology contributes meaningfully to historical discovery and cultural\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\nhighlighting their strengths and identifying areas for improvement. Our goal is\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\nthat technological advancements contribute meaningfully to historical\ndiscovery. Our code is available at:\n\\url{https://github.com/mbzuai-oryx/TimeTravel}.",
    "arxiv_id": "2502.14865v1",
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "format": "standard",
    "key_findings": "Among closed-source models, GPT- 4o-0806 achieved the highest BLEU (0.1758), ROUGE-L (0.1230), SPICE (0.1035), BERTScore (0.8349), and LLM-Judge score (0.3013), indi- cating superior semantic alignment and contex- tual richness.\nHowever, its lower METEOR score (0.2439) suggests that while it generates highly structured descriptions, they may lack word-level diversity and fluency.\nGPT-4o-mini-0718, de- spite scoring slightly lower in BLEU (0.1369) and ROUGE-L (0.1027), outperformed all models in METEOR (0.2658), highlighting its strength in producing more lexically diverse and well-formed outputs.",
    "technical_innovation": "Understanding historical and cultural artifacts demands human expertise and\nadvanced computational techniques, yet the process remains complex and\ntime-intensive.\nWhile large multimodal models offer promising support, their\nevaluation and improvement require a standardized benchmark.",
    "practical_applications": "No explicit applications extracted",
    "impact_analysis": "Our analysis highlights the potential of LMMs in bridging gaps in histori- cal records while maintaining academic integrity.",
    "pdf_file": "output/paper_cache/2502.14865v1.pdf"
  },
  {
    "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
    "authors": [
      "Weilin Zhao",
      "Tengyu Pan",
      "Xu Han",
      "Yudi Zhang",
      "Ao Sun",
      "Yuxiang Huang",
      "Kaihuo Zhang",
      "Weilun Zhao",
      "Yuxuan Li",
      "Jianyong Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14856v1",
    "abstract": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2.",
    "arxiv_id": "2502.14856v1",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "format": "standard",
    "key_findings": "4.1\n90.32 171.03 1.89 × 188.69 2.09 × Conv.\n89.85 187.95 2.09 × 212.08 2.36 × RAG 83.18 159.37 1.92 × 178.64 2.15 × Math 89.75 196.34 2.19 × 237.96 2.65 × QA 90.58 155.10 1.71 × 182.59 2.02 × Summ.",
    "technical_innovation": "The experimen- tal results demonstrate that while our previous anal- ysis primarily focused on EAGLE-2, our method also shows effectiveness when applied to other rep- resentative speculative sampling approaches, ex- hibiting strong compatibility and user-friendliness across different implementations.",
    "practical_applications": "No explicit applications extracted",
    "impact_analysis": "No explicit impact statements extracted",
    "pdf_file": "output/paper_cache/2502.14856v1.pdf"
  },
  {
    "title": "Prompt-to-Leaderboard",
    "authors": [
      "Evan Frick",
      "Connor Chen",
      "Joseph Tennyson",
      "Tianle Li",
      "Wei-Lin Chiang",
      "Anastasios N. Angelopoulos",
      "Ion Stoica"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14855v1",
    "abstract": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l.",
    "arxiv_id": "2502.14855v1",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "format": "standard",
    "key_findings": "In Section  3.2 , we show that P2L leads to gains in human preference prediction that scale with model size and data.\nIn Section  3.2 , we show direct predictive performance on pairwise human preferences, as well as scaling behavior with data size and parameter count.\nIn Section  3.3 , we show P2L allows for optimal cost-efficient routing via the algorithm developed previously in Section  2.1.2 .",
    "technical_innovation": "Each model  m  ∈ [ M ]  has a coefficient θ ∗ ( z ) m , and the higher this coefficient is, the more likely model  m  beats any other model on the prompt  z .\nLastly, we note that this strategy of training LLMs to output coefficients of parametric statistical models will be generalized in Section  2.2 .",
    "practical_applications": "This work develops fundamental tools for granular and query-specific evaluations in all evaluation tasks.",
    "impact_analysis": "Although our experiments are largely based on Chatbot Arena, this is not the only evaluation that could benefit from P2L.",
    "pdf_file": "output/paper_cache/2502.14855v1.pdf"
  },
  {
    "title": "Dynamic Concepts Personalization from Single Videos",
    "authors": [
      "Rameen Abdal",
      "Or Patashnik",
      "Ivan Skorokhodov",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Daniel Cohen-Or",
      "Kfir Aberman"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14844v1",
    "abstract": "Personalizing generative text-to-image models has seen remarkable progress,\nbut extending this personalization to text-to-video models presents unique\nchallenges. Unlike static concepts, personalizing text-to-video models has the\npotential to capture dynamic concepts, i.e., entities defined not only by their\nappearance but also by their motion. In this paper, we introduce\nSet-and-Sequence, a novel framework for personalizing Diffusion Transformers\n(DiTs)-based generative video models with dynamic concepts. Our approach\nimposes a spatio-temporal weight space within an architecture that does not\nexplicitly separate spatial and temporal features. This is achieved in two key\nstages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\nunordered set of frames from the video to learn an identity LoRA basis that\nrepresents the appearance, free from temporal interference. In the second\nstage, with the identity LoRAs frozen, we augment their coefficients with\nMotion Residuals and fine-tune them on the full video sequence, capturing\nmotion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\nweight space that effectively embeds dynamic concepts into the video model's\noutput domain, enabling unprecedented editability and compositionality while\nsetting a new benchmark for personalizing dynamic concepts.",
    "arxiv_id": "2502.14844v1",
    "primary_category": "cs.GR",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "format": "standard",
    "key_findings": "No explicit findings extracted",
    "technical_innovation": "IP MP AP OP Ours  vs  DreamMix 87% 88% 98% 100% Ours  vs  LoRA-1 99% 95% 94% 100% Ours  vs  LoRA-8 (DB-LoRA) 78% 75% 98% 98% Ours  vs  Two-Stage 86% 97% 76% 90% While our framework achieves state-of-the-art performance in video personalization and dynamic concept modeling, it does have lim- itations.\nAdditionally, while the method captures most motions with high fidelity, it may struggle with high-frequency or highly com- plex motion patterns, such as erratic or rapid movements, where temporal consistency could be further improved.",
    "practical_applications": "No explicit applications extracted",
    "impact_analysis": "No explicit impact statements extracted",
    "pdf_file": "output/paper_cache/2502.14844v1.pdf"
  },
  {
    "title": "Generating $π$-Functional Molecules Using STGG+ with Active Learning",
    "authors": [
      "Alexia Jolicoeur-Martineau",
      "Yan Zhang",
      "Boris Knyazev",
      "Aristide Baratin",
      "Cheng-Hao Liu"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14842v1",
    "abstract": "Generating novel molecules with out-of-distribution properties is a major\nchallenge in molecular discovery. While supervised learning methods generate\nhigh-quality molecules similar to those in a dataset, they struggle to\ngeneralize to out-of-distribution properties. Reinforcement learning can\nexplore new chemical spaces but often conducts 'reward-hacking' and generates\nnon-synthesizable molecules. In this work, we address this problem by\nintegrating a state-of-the-art supervised learning method, STGG+, in an active\nlearning loop. Our approach iteratively generates, evaluates, and fine-tunes\nSTGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We\napply STGG+AL to the design of organic $\\pi$-functional materials, specifically\ntwo challenging tasks: 1) generating highly absorptive molecules characterized\nby high oscillator strength and 2) designing absorptive molecules with\nreasonable oscillator strength in the near-infrared (NIR) range. The generated\nmolecules are validated and rationalized in-silico with time-dependent density\nfunctional theory. Our results demonstrate that our method is highly effective\nin generating novel molecules with high oscillator strength, contrary to\nexisting methods such as reinforcement learning (RL) methods. We open-source\nour active-learning code along with our Conjugated-xTB dataset containing 2.9\nmillion $\\pi$-conjugated molecules and the function for approximating the\noscillator strength and absorption wavelength (based on sTDA-xTB).",
    "arxiv_id": "2502.14842v1",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "format": "standard",
    "key_findings": "No explicit findings extracted",
    "technical_innovation": "Generating novel molecules with out-of-distribution properties is a major\nchallenge in molecular discovery.\nReinforcement learning can\nexplore new chemical spaces but often conducts 'reward-hacking' and generates\nnon-synthesizable molecules.",
    "practical_applications": "No explicit applications extracted",
    "impact_analysis": "No explicit impact statements extracted",
    "pdf_file": "output/paper_cache/2502.14842v1.pdf"
  },
  {
    "title": "Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning",
    "authors": [
      "Arun Sharma",
      "Majid Farhadloo",
      "Mingzhou Yang",
      "Ruolei Zeng",
      "Subhankar Ghosh",
      "Shashi Shekhar"
    ],
    "published": "2025-02-20",
    "url": "http://arxiv.org/pdf/2502.14840v1",
    "abstract": "Given inputs of diverse soil characteristics and climate data gathered from\nvarious regions, we aimed to build a model to predict accurate land emissions.\nThe problem is important since accurate quantification of the carbon cycle in\nagroecosystems is crucial for mitigating climate change and ensuring\nsustainable food production. Predicting accurate land emissions is challenging\nsince calibrating the heterogeneous nature of soil properties, moisture, and\nenvironmental conditions is hard at decision-relevant scales. Traditional\napproaches do not adequately estimate land emissions due to\nlocation-independent parameters failing to leverage the spatial heterogeneity\nand also require large datasets. To overcome these limitations, we proposed\nSpatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML),\nwhich leverages location-dependent parameters that account for significant\nspatial heterogeneity in soil moisture from multiple sites within the same\nregion. Experimental results demonstrate that SDSA-KGML models achieve higher\nlocal accuracy for the specified states in the Midwest Region.",
    "arxiv_id": "2502.14840v1",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "format": "standard",
    "key_findings": "This study demonstrates SDSA-KGML model employs location-based parameter values to effectively capture spa- tial heterogeneity.",
    "technical_innovation": "By contrast, the proposed approach (SDSA-KGML) incorpo- rates location dependence into the model itself.\nSDSA-KGML Framework:  The proposed approach leverages KGML-Ag architecture (Liu et al.",
    "practical_applications": "No explicit applications extracted",
    "impact_analysis": "This study demonstrates SDSA-KGML model employs location-based parameter values to effectively capture spa- tial heterogeneity.\nResults demonstrated that models trained on data specific to individual states better accuracy than those using location-independent parameters.",
    "pdf_file": "output/paper_cache/2502.14840v1.pdf"
  }
]