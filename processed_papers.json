{
  "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
  "authors": [
    "Yixin Song",
    "Zeyu Mi",
    "Haotong Xie",
    "Haibo Chen"
  ],
  "published": "2023-12-16",
  "url": "http://arxiv.org/pdf/2312.12456v2",
  "abstract": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key principle underlying the design of PowerInfer is\nexploiting the high locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution indicates that a\nsmall subset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. The evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance\ncomparable to that of a high-end server-grade A100 GPU, reaching 82% of its\ntoken generation rate on a single consumer-grade RTX 4090 GPU.",
  "arxiv_id": "2312.12456v2",
  "primary_category": "cs.LG",
  "categories": [
    "cs.LG",
    "cs.OS"
  ],
  "format": "standard",
  "key_findings": "8.1\nAll experiments were conducted on two distinct PC configurations, representing both high-end and low-end hardware scenarios: •  PC-High : Intel i9-13900K processor (eight 5.4GHz cores), 192GB host memory (bandwidth of 67.2 GB/s), an NVIDIA RTX 4090 GPU (24GB), and PCIe 4.0 inter- face (64GB/s bandwidth).\n•  PC-Low : Intel i7-12700K processor (eight 4.9GHz cores), 64GB host memory (bandwidth 38.4 GB/s), an NVIDIA RTX 2080Ti GPU (11GB), and PCIe 3.0 inter- face (32GB/s bandwidth).",
  "technical_innovation": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU.\nThe evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU.",
  "practical_applications": "It utilizes adaptive predictors and neuron-aware operators for neuron activation and computational sparsity.",
  "impact_analysis": "No explicit impact statements extracted",
  "pdf_file": "paper_cache/2312.12456v2.pdf"
}